{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import mtcnn\n",
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "detector=MTCNN()\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import ZeroPadding2D,Convolution2D,MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense,Dropout,Softmax,Flatten,Activation,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "import tensorflow.keras.backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 17, 16, 31, 6, 196897)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define VGG_FACE_MODEL architecture\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_face=Model(inputs=model.layers[0].input,outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model=tf.keras.models.load_model('home_face_classifier_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_path='D:/data/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_rep={0: 'ayush', 1: 'unique', 2: 'utkarsh'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(img):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.imshow(img[:,:,::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_extractor(img):\n",
    "    faces=face_cascade.detectMultiScale(img,1.3,5)\n",
    "    if faces in ():\n",
    "        return None\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        cropped_face=img[y:y+h,x:x+w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture=cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_record_ayush=[]\n",
    "time_record_unique=[]\n",
    "time_record_utkarsh=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    _,img=video_capture.read()\n",
    "\n",
    "    result=detector.detect_faces(img)\n",
    "# in the result we will have multiple number of dictionaries for each image, and we need to extract the bounding box of the face.      \n",
    "    for x in result:\n",
    "        bounding_box=x['box']\n",
    "#after extracting bounding box, draw rectangle around the face.\n",
    "        cv2.rectangle(img,(bounding_box),(0,0,255),3)\n",
    "#save the image.\n",
    "        x1,y3,w,h=x['box']\n",
    "        x2,y2=x1+w,y3+h\n",
    "        cropped=img[y3:y2, x1:x2]\n",
    "        if type(cropped) is np.ndarray:\n",
    "            img_c=cv2.resize(cropped,(224,224))\n",
    "        cv2.imwrite('D:/data/temp/crop_img.jpg',img_c)\n",
    "            # Get Embeddings\n",
    "        crop_img=load_img('D:/data/temp/crop_img.jpg',target_size=(224,224))\n",
    "        crop_img=img_to_array(crop_img)\n",
    "        crop_img=np.expand_dims(crop_img,axis=0)\n",
    "        crop_img=preprocess_input(crop_img)\n",
    "        img_encode=vgg_face(crop_img)\n",
    "        embed=K.eval(img_encode)\n",
    "        person=classifier_model.predict(embed)\n",
    "        if person[0][0]>.80:\n",
    "            name=person_rep[0]\n",
    "        elif person[0][1]>.80:\n",
    "            name=person_rep[1]\n",
    "        elif person[0][2]>.90:\n",
    "            name=person_rep[2]\n",
    "        else:\n",
    "            name='unknown'\n",
    "        os.remove('D:/data/temp/crop_img.jpg')\n",
    "        cv2.rectangle(img,(bounding_box),(0,0,255),3)\n",
    "        img=cv2.putText(img,name,(x1,y3-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,255),2,cv2.LINE_AA)\n",
    "        img=cv2.putText(img,str(np.max(person)),(x2,y2+10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "        if name !='unknown':\n",
    "            if name== 'ayush':\n",
    "                time_record_ayush.append(str(datetime.now()))\n",
    "            elif name=='unique':\n",
    "                time_record_unique.append(str(datetime.now()))\n",
    "            elif name=='utkarsh':\n",
    "                time_record_utkarsh.append(str(datetime.now()))\n",
    "            #time_record={name:}\n",
    "        else:\n",
    "            pass\n",
    "  # Save images with bounding box,name and accuracy \n",
    "    cv2.imshow('video',img)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "time_stamps={'ayush':time_record_ayush,'unique':time_record_unique,'utkarsh':time_record_utkarsh}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ayush': [],\n",
       " 'unique': ['2020-01-17 16:31:45.884005'],\n",
       " 'utkarsh': ['2020-01-17 16:31:31.174355',\n",
       "  '2020-01-17 16:31:36.736503',\n",
       "  '2020-01-17 16:31:37.304603',\n",
       "  '2020-01-17 16:31:37.889483',\n",
       "  '2020-01-17 16:31:38.609939',\n",
       "  '2020-01-17 16:31:44.661006',\n",
       "  '2020-01-17 16:31:48.846465',\n",
       "  '2020-01-17 16:31:49.531955',\n",
       "  '2020-01-17 16:31:50.754927',\n",
       "  '2020-01-17 16:31:51.353478',\n",
       "  '2020-01-17 16:31:53.110488',\n",
       "  '2020-01-17 16:31:53.694404']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "time_stamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
